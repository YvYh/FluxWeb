{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bienvenue dans Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YvYh/FluxWeb/blob/main/Model_Controverses2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7JxozYqb4u"
      },
      "source": [
        "# **BQ**: Get/Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoWR0fEEVc2j"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "json_path = 'poc-bigdata.json'\n",
        "bigquery_client = bigquery.Client.from_service_account_json(json_path)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mxZ0VONVJ8y"
      },
      "source": [
        "def get_bq_data(query):\n",
        "  query_job = bigquery_client.query(query)\n",
        "  rows = query_job.result()\n",
        "  data = rows.to_dataframe()\n",
        "  return data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14I5mpYIVr4X"
      },
      "source": [
        "def bq_load_df(name, df):\n",
        "    dataset_ref = bigquery_client.dataset('FluxWeb_Prediction')\n",
        "    table_ref = dataset_ref.table(name)\n",
        "    \n",
        "    \n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "    if name == 'Controverses_bert':\n",
        "      job_config.schema = get_bigquery_schema()\n",
        "    else:\n",
        "      job_config.autodetect=True\n",
        " \n",
        "    load_job = bigquery_client.load_table_from_dataframe(\n",
        "        df,\n",
        "        table_ref,\n",
        "        job_config=job_config)\n",
        " \n",
        "    assert load_job.job_type == 'load'\n",
        " \n",
        "    load_job.result()  # Waits for table load to complete.\n",
        " \n",
        "    assert load_job.state == 'DONE'\n",
        "    print('table {} load {} data.'.format(name, len(df)))"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBKdP9-8pfuW"
      },
      "source": [
        "# **TensorFlow**:  TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTVq5YN56-Fc"
      },
      "source": [
        "### Get Input Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nvtPr9q7tnu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Ln9vXYpkRX"
      },
      "source": [
        " \"\"\"SELECT DISTINCT NumControverse as label, Titre as Text\n",
        "FROM `poc-bigdata.FluxWeb_Prediction.Controverses_prd`\n",
        "UNION ALL \n",
        "SELECT DISTINCT NumControverse as label, Commentaire as Text\n",
        "FROM `poc-bigdata.FluxWeb_Prediction.Controverses_prd`\n",
        "WHERE Commentaire is not null and length(Commentaire)>5\n",
        "UNION ALL \n",
        "SELECT DISTINCT NumControverse as label, Informations as Text\n",
        "FROM `poc-bigdata.FluxWeb_Prediction.Controverses_prd`\n",
        "WHERE Informations is not null\n",
        "\"\"\"\n",
        "q=\"SELECT label, Text FROM `poc-bigdata.FluxWeb_Prediction.Controverse_dataset`\"\n",
        "controverses = get_bq_data(q)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqtPfh_EpdkM"
      },
      "source": [
        "#controverses['Text_clean']=controverses.Text.apply(text_preprossing)\n",
        "controverses = controverses[['label','Text']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "MSdOrlfrz95b",
        "outputId": "657cf06d-a324-4e1b-ec83-34c0de540f54"
      },
      "source": [
        "print(len(controverses))\n",
        "print(len(controverses.label.unique()))\n",
        "controverses.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11537\n",
            "1408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2866</td>\n",
              "      <td>Révélation d'un scandale comptable survenu en ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>389</td>\n",
              "      <td>EPR de Flamanville: malfaçons</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2543</td>\n",
              "      <td>Condamnation suite à des déversements dans des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3506</td>\n",
              "      <td>Poursuites aux Etats-Unis en lien avec les émi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3016</td>\n",
              "      <td>Incendie dans une usine de Lubrizol à Rouen</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               Text\n",
              "0   2866  Révélation d'un scandale comptable survenu en ...\n",
              "1    389                      EPR de Flamanville: malfaçons\n",
              "2   2543  Condamnation suite à des déversements dans des...\n",
              "3   3506  Poursuites aux Etats-Unis en lien avec les émi...\n",
              "4   3016        Incendie dans une usine de Lubrizol à Rouen"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xxz936WnQPs",
        "outputId": "1f7c74ef-76d3-47d2-f60a-a2e439c172f6"
      },
      "source": [
        "controverses.label.sort_values().unique()"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  23,   57,   64, ..., 3613, 3614, 3615])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T41MC8evoEWV"
      },
      "source": [
        "label_index = {k: v for v, k in enumerate(controverses.label.sort_values().unique())}\n",
        "controverses.label = controverses.label.map(lambda x: label_index.get(x))"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYLWAp_aqyX5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b284ecd-6b3c-4669-b695-31ae6207099e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(controverses, test_size=0.04)\n",
        "train, val = train_test_split(train, test_size=0.1)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9967 train examples\n",
            "1108 validation examples\n",
            "462 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ5e1rs7tvpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "6bdfc0dc-5b7e-4bda-f6db-ba54f18255f6"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7622</th>\n",
              "      <td>3571</td>\n",
              "      <td>Tennet a répondu qu'il est impossible de fixer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2581</th>\n",
              "      <td>2777</td>\n",
              "      <td>La Cour Suprême en Ukraine a rejeté l'appel de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3469</th>\n",
              "      <td>2508</td>\n",
              "      <td>Ce n'est pas seulement Amazon qui pose problèm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9925</th>\n",
              "      <td>3049</td>\n",
              "      <td>Les cinq principaux prêteurs identifiés dans l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3366</th>\n",
              "      <td>2545</td>\n",
              "      <td>Ils ont indiqué qu'ils aimeraient résoudre ce ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               Text\n",
              "7622   3571  Tennet a répondu qu'il est impossible de fixer...\n",
              "2581   2777  La Cour Suprême en Ukraine a rejeté l'appel de...\n",
              "3469   2508  Ce n'est pas seulement Amazon qui pose problèm...\n",
              "9925   3049  Les cinq principaux prêteurs identifiés dans l...\n",
              "3366   2545  Ils ont indiqué qu'ils aimeraient résoudre ce ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNSuVrS9rD5L"
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  #dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('label')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dataframe.values, labels.values))\n",
        "  #ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNdFIf0Asv-Z"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 32\n",
        "train_ds = df_to_dataset(train)\n",
        "test_ds = df_to_dataset(test)\n",
        "val_ds= df_to_dataset(val)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkpYs1_6s6rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388d81f2-dbc3-48a6-f56a-f5922816e397"
      },
      "source": [
        "for example, label in train_ds.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texts:  [[b\"Un porte-parole d'Hindustan Unilever a admis avoir achet\\xc3\\xa9 du th\\xc3\\xa9 en 2018 dans deux plantations non accr\\xc3\\xa9dit\\xc3\\xa9es par la Rainforest Alliance dans l'\\xc3\\x89tat indien d'Assam\"]\n",
            " [b\"De plus en plus de donn\\xc3\\xa9es (26 421 plaintes de patients) indiquent que les pompes \\xc3\\xa0 insuline Medtronic MiniMed 600 Series auraient eu des dysfonctionnements techniques majeurs, blessant plus de 2000 personnes et causant la mort d'un patient\"]\n",
            " [b\"N\\xc3\\xa9anmoins pour l'instant, le groupe a r\\xc3\\xa9agi et ces probl\\xc3\\xa8mes semblent limit\\xc3\\xa9s.\"]]\n",
            "\n",
            "labels:  [ 624 1082  392]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZPywzqc2sof"
      },
      "source": [
        "### Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6AlgtWqsD7q"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download fr_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBzAA6MZ3Y9I"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_lg\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp-fCZ9YrIgf"
      },
      "source": [
        "def text_preprossing(text):\n",
        "    tokens = nlp(text)\n",
        "    clean = []\n",
        "    for token in tokens:\n",
        "        if not any([token.is_digit, token.is_punct, token.is_currency,\n",
        "               token.like_url, token.like_num, token.like_email,\n",
        "               token.is_stop]):\n",
        "            clean += [token.lemma_]\n",
        "    return tf.strings.join(clean, separator=' ')\n",
        "    #return ' '.join(clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYhU3yhg7ysF"
      },
      "source": [
        "-------  \n",
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yf38V0Spgg3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqSEtyDksvBK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGjzxZGsBxBM"
      },
      "source": [
        "text_list = train.Text.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0k6GOBN50x8"
      },
      "source": [
        "vectorize_layer.adapt(text_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i4oz_7nInN2"
      },
      "source": [
        "vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoTERsbu3sRX"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBFZEV8YHyAA"
      },
      "source": [
        "\n",
        "embedding_dim=1000\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer,\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(16, activation='relu'),\n",
        "  Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUf31CwRH7Ng"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaKzMiSM6k8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b87fa6-c7b0-4790-fda5-e89da133fc6e"
      },
      "source": [
        "model.fit(train_ds, \n",
        "          epochs=40, \n",
        "          validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 3s 26ms/step - loss: -779.2183 - accuracy: 0.0000e+00 - val_loss: -1786.3885 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -4096.7046 - accuracy: 0.0000e+00 - val_loss: -7218.8159 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -13170.0039 - accuracy: 0.0000e+00 - val_loss: -20210.8047 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -31861.1738 - accuracy: 0.0000e+00 - val_loss: -44427.7812 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -63834.3125 - accuracy: 0.0000e+00 - val_loss: -83342.9688 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -112532.9219 - accuracy: 0.0000e+00 - val_loss: -140196.9688 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -181148.9375 - accuracy: 0.0000e+00 - val_loss: -217976.3750 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -272621.8750 - accuracy: 0.0000e+00 - val_loss: -319431.0625 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -389657.6562 - accuracy: 0.0000e+00 - val_loss: -447080.0625 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -534746.1875 - accuracy: 0.0000e+00 - val_loss: -603253.3125 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5a0eef3910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeSaSkfTILKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3045297b-fa41-4e8a-a147-5e98e1bf3bae"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_vectorization_5 (TextVe (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 10000, 16)         160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u88jdipb6nRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d164790-80f1-4306-a038-bdf21a046521"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 9ms/step - loss: -602432.0625 - accuracy: 0.0000e+00\n",
            "Accuracy 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eURBDR2KKORo"
      },
      "source": [
        "#### tfidf model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRq0QLFCLAWo"
      },
      "source": [
        "def text_preprossing2(text):\n",
        "    tokens = nlp(text)\n",
        "    clean = []\n",
        "    for token in tokens:\n",
        "        if not any([token.is_digit, token.is_punct, token.is_currency,\n",
        "               token.like_url, token.like_num, token.like_email,\n",
        "               token.is_stop]):\n",
        "            clean += [token.lemma_]\n",
        "    return tf.strings.join(clean, separator=' ')\n",
        "    #return ' '.join(clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nyk7GW7K_uO"
      },
      "source": [
        "vectorize_layer2 = TextVectorization(\n",
        "    #standardize= text_preprossing,\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='tf-idf',\n",
        "    #output_sequence_length=sequence_length\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6SLYUjLJGN"
      },
      "source": [
        "vectorize_layer2.adapt(data.Text.values.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSPsmJTnKNig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b1030b-a8d2-4d05-e17d-7cd6dc631a4f"
      },
      "source": [
        "tfidf = tf.keras.models.Sequential()\n",
        "tfidf.add(tf.keras.Input(shape=(1,),dtype=tf.string))\n",
        "tfidf.add(vectorize_layer2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iECWOpVNMzh"
      },
      "source": [
        "tfidf.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tfidf.fit(train.Text.values.tolist(), \n",
        "          epochs=40, \n",
        "          validation_data=val.Text.values.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Wm1OCmL0JU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac74213-8bd2-44fd-e2c4-bdc9871b6ce6"
      },
      "source": [
        "print(test.Text.values[0])\n",
        "tfidf.predict([test.Text.values[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "US Equal Employment Opportunity commission EEOC intenter action justice tribunal fédéral NY contre Walmart enfreindre loi fédéral laisser employé masculin harceler sexuellement collègue travail prendre mesure cesse lieu mettre fin harcèlement Walmart demander employé harceler défendre contraindre salarié démissionner agence fédéral affirme walmart recevoir plainte sujet comportement harcelant homme partir entreprise prendre mesure efficace faire cesser harcèlement eeoc réclame arriéré salaire dommage intérêt compensatoire dommage intérêt punitif employé concerner mesure redressement prévenir potentiel cas harcèlement sexuel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.586464, 0.      , 0.      , ..., 0.      , 0.      , 0.      ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUh5-B936p9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c525bf-1800-402c-ac0a-4a93dfbb7efd"
      },
      "source": [
        "tfidf.save('tf_idf_model')\n",
        "#reloaded_model = tf.keras.models.load_model('my_pet_classifier')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: tf_idf_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCijeiB6zP7"
      },
      "source": [
        "news =''\n",
        "\n",
        "input_dict = {'Text': tf.convert_to_tensor([news])}\n",
        "predictions = reloaded_model.predict(input_dict)\n",
        "prob = tf.nn.sigmoid(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This particular pet had a %.1f percent probability \"\n",
        "    \"of getting adopted.\" % (100 * prob)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmOFNNwGVtct"
      },
      "source": [
        "### Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR6p1S-8VsfN"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woMyx2sCZaVb",
        "outputId": "cb6b8e1b-1b14-4520-8bff-86f423f1cf4f"
      },
      "source": [
        "train_text = train_ds.map(lambda text,label: text)\n",
        "train_text"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<MapDataset shapes: (None, 1), types: tf.string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncars-ZOV33O"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(10):\n",
        "    print(\"Text: \", text_batch.numpy()[i])\n",
        "    print(\"Label:\", label_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrQD9fYuWCKM"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download fr_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wji5qwMXdmN"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_lg\")"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv2kG-7mXgAt"
      },
      "source": [
        "def get_text(t: tf.Tensor):\n",
        "  return t.numpy().decode('utf-8')\n",
        "def text_preprossing(text: np.ndarray):\n",
        "  result = []\n",
        "  for t in text:\n",
        "    tokens = nlp(t.decode('utf-8'))\n",
        "    clean = []\n",
        "    for token in tokens:\n",
        "        if not any([token.is_digit, token.is_punct, token.is_currency,\n",
        "              token.like_url, token.like_num, token.like_email,\n",
        "              token.is_stop]):\n",
        "            clean += [token.lemma_]\n",
        "    #result = result+ [[tf.strings.join(clean, separator=' ')]]\n",
        "    result = result+ [[' '.join(clean)]]\n",
        "    #return ' '.join(clean)\n",
        "    print(result)\n",
        "  return tf.constant(result)\n",
        "def clean_text(t: tf.Tensor):\n",
        "  return tf.py_function(func=text_preprossing, inp=[t], Tout=tf.string)"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_X25xR7hzqN"
      },
      "source": [
        "tf.config.run_functions_eagerly(False)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEb96u4feOTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd5376eb-2a0c-40cb-e525-efbe36d44611"
      },
      "source": [
        "test_text = test_ds.map(lambda text,label: text)\n",
        "test = next(iter(test_text))\n",
        "print(test)\n",
        "clean_text(test)"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[b\"Ces enqu\\xc3\\xaates portaient sur l'embauche d'enfants de personnalit\\xc3\\xa9s chinoises et visaient plusieurs banques, soup\\xc3\\xa7onn\\xc3\\xa9es de les avoir recrut\\xc3\\xa9s, dans l'espoir d'obtenir des contrats\"]\n",
            " [b\"Apr\\xc3\\xa8s des \\xc3\\xa9meutes en 2016, d'autres violations ont eu lieu dans ce centre mais dans d'autres \\xc3\\xa9galement\"]\n",
            " [b'Quatre groupes indig\\xc3\\xa8nes ont demand\\xc3\\xa9 \\xc3\\xa0 Frontera de nettoyer les dommages environnementaux li\\xc3\\xa9s aux d\\xc3\\xa9versements de p\\xc3\\xa9trole du Lot 192, le plus grand puits de p\\xc3\\xa9trole du P\\xc3\\xa9rou']\n",
            " [b\"Edison a vers\\xc3\\xa9 500 $ par habitant et une enqu\\xc3\\xaate est en cours pour d\\xc3\\xa9terminer les causes de l'accident\"]\n",
            " [b\"La CFTC (US Commodity Futures Trading Commission) avait d\\xc3\\xa9pos\\xc3\\xa9 plainte contre Kraft Foods et Mondelez en Illinois en avril 2015, all\\xc3\\xa9guant que les soci\\xc3\\xa9t\\xc3\\xa9s avaient manipul\\xc3\\xa9 les prix du bl\\xc3\\xa9 (options) et du bl\\xc3\\xa9 (futures) \\xc3\\xa0 partir de fin 2011, r\\xc3\\xa9alisant des b\\xc3\\xa9n\\xc3\\xa9fices d'environ 5,4 M USD\"]\n",
            " [b'La banque encourt une amende 5Mds?, correspondant \\xc3\\xa0 la moiti\\xc3\\xa9 des montants sur lesquels portent les op\\xc3\\xa9rations pr\\xc3\\xa9sum\\xc3\\xa9es frauduleuses.']\n",
            " [b'Les maisons, champs, ressources en eau et les sites culturels et religieux des r\\xc3\\xa9sidents locaux auraient \\xc3\\xa9t\\xc3\\xa9 d\\xc3\\xa9truits']\n",
            " [b\"Dans un rapport publi\\xc3\\xa9 mi-septembre, l'ONG Solidar Suisse d\\xc3\\xa9taille les violations des droits de l'homme qu'elle a constat\\xc3\\xa9es dans deux plantations d'huile de palme fournissant, selon elle, Nestl\\xc3\\xa9 en Malaisie (Etat de Sabah sans que les plantations pr\\xc3\\xa9cises n'aient \\xc3\\xa9t\\xc3\\xa9 r\\xc3\\xa9v\\xc3\\xa9l\\xc3\\xa9es)\"]\n",
            " [b'Le CEO Don Katz semble vouloir changer cela en prenant des mesures et en appelant les collaborateurs \\xc3\\xa0 rapporter tout comportement inappropri\\xc3\\xa9']\n",
            " [b'Un nouveau jugement par une Cour sup\\xc3\\xa9rieure pourrait intervenir dans les 2 ans']\n",
            " [b'De plus, une enqu\\xc3\\xaate, toujours en cours, a \\xc3\\xa9t\\xc3\\xa9 ouverte en avril 2018 par les Forces arm\\xc3\\xa9es royales du Maroc']\n",
            " [b\"En 2018 Eni a affirm\\xc3\\xa9 devant le tribunal qu'il n'y a aucun lien entre la pollution d'origine industrielle et les cas de malformation n\\xc3\\xa9onatale\"]\n",
            " [b'Cette affaire continue de peser lourdement sur la r\\xc3\\xa9putation du groupe ainsi que sur son cours de bourse et ses b\\xc3\\xa9n\\xc3\\xa9fices']\n",
            " [b\"Le groupe L'Or\\xc3\\xa9al a annonc\\xc3\\xa9 qu'il allait verser un total de 320 millions d'euros au fisc fran\\xc3\\xa7ais pour \\xc2\\xab r\\xc3\\xa9gler un diff\\xc3\\xa9rend \\xc2\\xbb sur la base imposable de certaines de ses filiales dont Lanc\\xc3\\xb4me, concernant principalement l'imp\\xc3\\xb4t sur les soci\\xc3\\xa9t\\xc3\\xa9s r\\xc3\\xa9gl\\xc3\\xa9 entre 2014 et 2018\"]\n",
            " [b'Abus de position dominante \\xc3\\xa0 Taiwan: une amende largement r\\xc3\\xa9duite pour Qualcomm']\n",
            " [b\"Les deux directeurs (le Chief Content Officer et le Chief Revenue Officer) ont brusquement quitt\\xc3\\xa9 le groupe en plein milieu d'une enqu\\xc3\\xaate, qui fait suite \\xc3\\xa0 un questionnaire rempli par les collaborateurs d'Audible\"]\n",
            " [b\"L'UE a sanctionn\\xc3\\xa9 Qualcomm d'une amende de 242M? pour ses pratiques anticoncurrentielles\"]\n",
            " [b'La contamination avait \\xc3\\xa9t\\xc3\\xa9 d\\xc3\\xa9couverte pour la premi\\xc3\\xa8re fois en avril 2017, mais la communaut\\xc3\\xa9 en avait \\xc3\\xa9t\\xc3\\xa9 inform\\xc3\\xa9e en ao\\xc3\\xbbt 2017']\n",
            " [b'Le Dengvaxia a, par ailleurs, \\xc3\\xa9t\\xc3\\xa9 autoris\\xc3\\xa9 \\xc3\\xa0 la vente en Europe en d\\xc3\\xa9cembre 2018 pour des personnes ayant d\\xc3\\xa9j\\xc3\\xa0 \\xc3\\xa9t\\xc3\\xa9 infect\\xc3\\xa9es par le virus']\n",
            " [b\"Le groupe a accept\\xc3\\xa9 de collaborer au d\\xc3\\xa9but de l'enqu\\xc3\\xaate ce qui lui a permis de diminuer de 30% le montant de la p\\xc3\\xa9nalit\\xc3\\xa9 qu'elle risquait initialement de recevoir.\"]\n",
            " [b\"Le parquet fait \\xc3\\xa9tat du t\\xc3\\xa9moignage d'une personne qui aurait \\xc3\\xa9t\\xc3\\xa9 mise au courant de l'article du FT avant sa publication\"]\n",
            " [b\"Il aurait en effet identifi\\xc3\\xa9 365 comptes en Su\\xc3\\xa8de, au Danemark, en Finlande et en Norv\\xc3\\xa8ge sur lesquels auraient transit\\xc3\\xa9 175M$ afin de blanchir de l'argent sale en provenance de Russie entre 2007 et 2013\"]\n",
            " [b\"La BaFin a indiqu\\xc3\\xa9 enqu\\xc3\\xaater sur Grenke et d'\\xc3\\xa9ventuelles manipulations de march\\xc3\\xa9s autour de cette affaire\"]\n",
            " [b'Une nouvelle poursuite a \\xc3\\xa9t\\xc3\\xa9 intent\\xc3\\xa9e devant une cour am\\xc3\\xa9ricaine contre Johnson & Johnson et Bayer HealthCare concernant les effets mortels du coagulant Xarelto']\n",
            " [b\"Le projet \\xc2\\xab Proyecto Integrales Morelos \\xc2\\xbb (PIM) est critiqu\\xc3\\xa9 pour violation des droits de l'homme au Mexique.\"]\n",
            " [b'A titre de rappel, la soci\\xc3\\xa9t\\xc3\\xa9 \\xc3\\xa9tait en probation depuis 2017 pour crimes environnementaux et avait pay\\xc3\\xa9 une amende de 40M$']\n",
            " [b\"BP a aussi \\xc3\\xa9t\\xc3\\xa9 accus\\xc3\\xa9e d'avoir revendu les barils achet\\xc3\\xa9s par HLT \\xc3\\xa0 cette m\\xc3\\xaame compagnie, afin de faire croire que HLT disposait de liquidit\\xc3\\xa9s importantes\"]\n",
            " [b'Dans le cadre de conflits sociaux dans deux usines de ces groupes, la police serait intervenue se traduisant par des bless\\xc3\\xa9s et des arrestations']\n",
            " [b\"Daimler reste \\xc3\\xa0 risque quant au diesel : le groupe reste peu transparent et d'autres condamnations le concernant sont attendues.\"]\n",
            " [b\"Le syndicat reproche \\xc3\\xa0 la compagnie d'avoir voulu faire bonne figure en acceptant le dialogue sans r\\xc3\\xa9elles intentions de compromis derri\\xc3\\xa8re\"]\n",
            " [b\"Sanofi fait face \\xc3\\xa0 plusieurs poursuites judiciaires en Suisse accusant le m\\xc3\\xa9dicament anti-\\xc3\\xa9pileptique Depakine de causer des probl\\xc3\\xa8mes de sant\\xc3\\xa9 publique, notamment des troubles cognitifs, des malformations et le d\\xc3\\xa9veloppement de l'autisme chez les enfants de femmes sous traitement\"]\n",
            " [b\"L'enqu\\xc3\\xaate de l'UE, ouverte en 2015, faisait suite \\xc3\\xa0 une plainte du groupe Fairsearch en 2013\"]], shape=(32, 1), dtype=string)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-181-146fe2ea4ee5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-178-592a8b4e88df>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_preprossing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[0;34m(func, inp, Tout, name)\u001b[0m\n\u001b[1;32m    511\u001b[0m   \"\"\"\n\u001b[1;32m    512\u001b[0m   return _eager_py_func(\n\u001b[0;32m--> 513\u001b[0;31m       func=func, inp=inp, Tout=Tout, name=name, use_tape_cache=True)\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36m_eager_py_func\u001b[0;34m(func, inp, Tout, name, use_tape_cache)\u001b[0m\n\u001b[1;32m    418\u001b[0m           \u001b[0meager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m           use_tape_cache=use_tape_cache)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m   return _internal_py_func(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36m_internal_py_func\u001b[0;34m(func, inp, Tout, stateful, eager, is_grad_func, name, use_tape_cache)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mis_async\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    349\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[0;34m(input, token, Tout, is_async, name)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 247, in __call__\n    return func(device, token, args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py\", line 135, in __call__\n    ret = self._func(*args)\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-178-592a8b4e88df>\", line 6, in text_preprossing\n    tokens = nlp(t.decode('utf-8'))\n\n  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 401, in __getattr__\n    self.__getattribute__(name)\n\nAttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'\n\n [Op:EagerPyFunc]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRus14y-YN5F"
      },
      "source": [
        "vocab_size = 10000\n",
        "sequence_length = 600\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize= clean_text,\n",
        "    #standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='tf-idf',\n",
        "    #output_sequence_length=sequence_length\n",
        "    )"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "qql50W5XY2Wj",
        "outputId": "eea4634e-d8f4-4914-cb51-b1bf26c33bfa"
      },
      "source": [
        "vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-164-3945a2d006a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, reset_state)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_legacy_output_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m       \u001b[0mpreprocessed_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1923\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   1924\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 1925\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1926\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1927\u001b[0m       return ParallelMapDataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   4485\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4486\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4487\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   4488\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   4489\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3710\u001b[0m     \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3712\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3713\u001b[0m       \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3714\u001b[0m       \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3133\u001b[0m     \"\"\"\n\u001b[1;32m   3134\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m-> 3135\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   3136\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3098\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3099\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3100\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3101\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3102\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3685\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3686\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3687\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3688\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3615\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3616\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3617\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3618\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3619\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    525\u001b[0m       \u001b[0;31m# so can be squeezed out. We do this here instead of after splitting for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m       \u001b[0;31m# performance reasons - it's more expensive to squeeze a ragged tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSPLIT_ON_WHITESPACE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NmNWcUQgyN0"
      },
      "source": [
        "### Binary classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doxc0s-WZBUa"
      },
      "source": [
        "VOCAB_SIZE = 10000\n",
        "\n",
        "binary_vectorize_layer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='binary')\n",
        "train_text = train_ds.map(lambda text, labels: text)\n",
        "binary_vectorize_layer.adapt(train_text)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mrx-ohN7zmp"
      },
      "source": [
        "def binary_vectorize_text(text, label):\n",
        "  #text = tf.expand_dims(text, -1)\n",
        "  return binary_vectorize_layer(text), label"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uwv9pm0wjpaV",
        "outputId": "3f4eef89-cd74-4975-e658-8a3de3aaa610"
      },
      "source": [
        "# Retrieve a batch (of 32 reviews and labels) from the dataset\n",
        "text_batch, label_batch = next(iter(train_ds))\n",
        "first_text, first_label = text_batch[0], label_batch[0]\n",
        "print(\"Text\", first_text)\n",
        "print(\"Label\", first_label)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text tf.Tensor([b'D\\xc3\\xa8s lors le groupe devra payer 50000? par emploi non cr\\xc3\\xa9\\xc3\\xa9.'], shape=(1,), dtype=string)\n",
            "Label tf.Tensor(1623, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeo6RpeUjxWl",
        "outputId": "14d9b7fc-85e1-4ec6-a78d-db76c0a4abaf"
      },
      "source": [
        "print(\"'binary' vectorized question:\", \n",
        "      binary_vectorize_text(first_text, first_label)[0])"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'binary' vectorized question: tf.Tensor([[0. 0. 0. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02NkVUfjj2GW"
      },
      "source": [
        "binary_train_ds = train_ds.map(binary_vectorize_text)\n",
        "binary_val_ds = val_ds.map(binary_vectorize_text)\n",
        "binary_test_ds = test_ds.map(binary_vectorize_text)\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD3fkB0gi_WN",
        "outputId": "c9ed3608-23b3-4a70-8503-01a81606f51b"
      },
      "source": [
        "def configure_dataset(dataset):\n",
        "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "binary_train_ds = configure_dataset(binary_train_ds)\n",
        "binary_val_ds = configure_dataset(binary_val_ds)\n",
        "binary_test_ds = configure_dataset(binary_test_ds)\n",
        "binary_model = tf.keras.Sequential([layers.Dense(1408)])\n",
        "binary_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "history = binary_model.fit(\n",
        "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "312/312 [==============================] - 30s 93ms/step - loss: 7.0292 - accuracy: 0.0339 - val_loss: 6.7537 - val_accuracy: 0.0532\n",
            "Epoch 2/10\n",
            "312/312 [==============================] - 28s 90ms/step - loss: 5.1791 - accuracy: 0.3137 - val_loss: 6.3566 - val_accuracy: 0.1011\n",
            "Epoch 3/10\n",
            "312/312 [==============================] - 28s 90ms/step - loss: 3.8214 - accuracy: 0.7078 - val_loss: 6.1175 - val_accuracy: 0.1399\n",
            "Epoch 4/10\n",
            "312/312 [==============================] - 28s 90ms/step - loss: 2.8739 - accuracy: 0.8757 - val_loss: 5.9865 - val_accuracy: 0.1661\n",
            "Epoch 5/10\n",
            "312/312 [==============================] - 28s 89ms/step - loss: 2.2497 - accuracy: 0.9243 - val_loss: 5.9243 - val_accuracy: 0.1742\n",
            "Epoch 6/10\n",
            "312/312 [==============================] - 28s 90ms/step - loss: 1.8262 - accuracy: 0.9382 - val_loss: 5.9035 - val_accuracy: 0.1787\n",
            "Epoch 7/10\n",
            "312/312 [==============================] - 28s 91ms/step - loss: 1.5233 - accuracy: 0.9460 - val_loss: 5.9084 - val_accuracy: 0.1796\n",
            "Epoch 8/10\n",
            "312/312 [==============================] - 28s 89ms/step - loss: 1.2966 - accuracy: 0.9497 - val_loss: 5.9297 - val_accuracy: 0.1805\n",
            "Epoch 9/10\n",
            "312/312 [==============================] - 28s 90ms/step - loss: 1.1207 - accuracy: 0.9522 - val_loss: 5.9618 - val_accuracy: 0.1823\n",
            "Epoch 10/10\n",
            "312/312 [==============================] - 28s 89ms/step - loss: 0.9802 - accuracy: 0.9536 - val_loss: 6.0012 - val_accuracy: 0.1796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Alj0_5ESmc8S",
        "outputId": "221b21ac-26d4-45ba-f2b3-c0432d496f7e"
      },
      "source": [
        "print(\"Linear model on binary vectorized data:\")\n",
        "print(binary_model.summary())"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear model on binary vectorized data:\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 1408)              14081408  \n",
            "=================================================================\n",
            "Total params: 14,081,408\n",
            "Trainable params: 14,081,408\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vI10gp0PkERd",
        "outputId": "ac42d216-81e2-41b5-a56b-8d0457c6a3a8"
      },
      "source": [
        "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
        "print(\"Binary model accuracy: {:2.2%}\".format(binary_accuracy))"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 1s 29ms/step - loss: 6.2363 - accuracy: 0.1537\n",
            "Binary model accuracy: 15.37%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxhhEX6AqOsU",
        "outputId": "8196fbbd-f83c-40f7-abd9-24e32826aac5"
      },
      "source": [
        "export_model = tf.keras.Sequential(\n",
        "    [binary_vectorize_layer, binary_model,\n",
        "     layers.Activation('sigmoid')])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(test_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(binary_accuracy))"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 1s 33ms/step - loss: 6.2363 - accuracy: 0.1537\n",
            "Accuracy: 15.37%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXwntrLUqiSJ"
      },
      "source": [
        "def get_string_labels(predicted_scores_batch):\n",
        "  predicted_ints = tf.argmax(predicted_scores_batch, axis=1).numpy()\n",
        "  return [[k for k,v in label_index.items() if v == i][0] for i in predicted_ints]"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfBzku5mr5cD",
        "outputId": "2ac74a15-1ec4-4955-e254-7c2a3f6eb62b"
      },
      "source": [
        "predicted_int_labels.numpy()"
      ],
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWed_qZftFhe",
        "outputId": "a18453d0-ca3b-4170-e270-a89140a898b7"
      },
      "source": [
        "[k for k,v in label_index.items() if v == 16][0]"
      ],
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa_1xAJvrP2S",
        "outputId": "69ca3744-37b6-47e6-9a8a-a81302197065"
      },
      "source": [
        "inputs=[\"EPR de Flamanville: malfaçons\",\n",
        "        \"Redressement fiscal des GAFAM en France\"]\n",
        "predicted_scores = export_model.predict(inputs)\n",
        "predicted_labels = get_string_labels(predicted_scores)\n",
        "for input, label in zip(inputs, predicted_labels):\n",
        "  print(\"Text: \", input)\n",
        "  print(\"Predicted label: \", label)"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text:  EPR de Flamanville: malfaçons\n",
            "Predicted label:  389\n",
            "Text:  Redressement fiscal des GAFAM en France\n",
            "Predicted label:  2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI0C8Ymkwr1S",
        "outputId": "16fe9604-7c2b-4fb8-a28d-991cfdf24942"
      },
      "source": [
        "export_model.summary()"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_vectorization_16 (TextV (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 1408)              14081408  \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1408)              0         \n",
            "=================================================================\n",
            "Total params: 14,081,408\n",
            "Trainable params: 14,081,408\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC2PNBGCw2kb",
        "outputId": "b127a0ae-9e25-4f46-8c32-8df28768ce26"
      },
      "source": [
        "export_model.save('binaryClassif')"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: binaryClassif/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ERG_bJ3xQVY",
        "outputId": "ded43e6f-28b2-4435-e58a-70f7878a017e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "QNR08xRnyESn",
        "outputId": "e06b8962-52c7-4783-c59c-0fe29f83d7b9"
      },
      "source": [
        "pd.DataFrame({'label':label_index.values(), 'numControverse':label_index.keys()})"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>numControverse</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>1403</td>\n",
              "      <td>3611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1404</th>\n",
              "      <td>1404</td>\n",
              "      <td>3612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>1405</td>\n",
              "      <td>3613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1406</th>\n",
              "      <td>1406</td>\n",
              "      <td>3614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>1407</td>\n",
              "      <td>3615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1408 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      label  numControverse\n",
              "0         0              23\n",
              "1         1              57\n",
              "2         2              64\n",
              "3         3              69\n",
              "4         4              77\n",
              "...     ...             ...\n",
              "1403   1403            3611\n",
              "1404   1404            3612\n",
              "1405   1405            3613\n",
              "1406   1406            3614\n",
              "1407   1407            3615\n",
              "\n",
              "[1408 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynpLunx5ypLe",
        "outputId": "0133dc23-e1b4-4d78-de3d-39fa18387261"
      },
      "source": [
        "bq_load_df(\"LabelControv\", pd.DataFrame({'label':label_index.values(), 'numControverse':label_index.keys()}))"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "table LabelControv load 1408 data.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}