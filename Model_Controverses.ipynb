{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bienvenue dans Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YvYh/FluxWeb/blob/main/Model_Controverses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr7JxozYqb4u"
      },
      "source": [
        "# **BQ**: Get/Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoWR0fEEVc2j"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "json_path = 'poc-bigdata.json'\n",
        "bigquery_client = bigquery.Client.from_service_account_json(json_path)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mxZ0VONVJ8y"
      },
      "source": [
        "def get_bq_data(query):\n",
        "  query_job = bigquery_client.query(query)\n",
        "  rows = query_job.result()\n",
        "  data = rows.to_dataframe()\n",
        "  return data"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14I5mpYIVr4X"
      },
      "source": [
        "def bq_load_df(name, df):\n",
        "    dataset_ref = bigquery_client.dataset('FluxWeb_Prediction')\n",
        "    table_ref = dataset_ref.table(name)\n",
        "    \n",
        "    \n",
        "    job_config = bigquery.LoadJobConfig()\n",
        "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
        "    if name == 'Controverses_bert':\n",
        "      job_config.schema = get_bigquery_schema()\n",
        "    else:\n",
        "      job_config.autodetect=True\n",
        " \n",
        "    load_job = bigquery_client.load_table_from_dataframe(\n",
        "        df,\n",
        "        table_ref,\n",
        "        job_config=job_config)\n",
        " \n",
        "    assert load_job.job_type == 'load'\n",
        " \n",
        "    load_job.result()  # Waits for table load to complete.\n",
        " \n",
        "    assert load_job.state == 'DONE'\n",
        "    print('table {} load {} data.'.format(name, len(df)))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBKdP9-8pfuW"
      },
      "source": [
        "# **TensorFlow**:  TextVectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTVq5YN56-Fc"
      },
      "source": [
        "### Get Input Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nvtPr9q7tnu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4Ln9vXYpkRX"
      },
      "source": [
        "q = \"\"\"SELECT DISTINCT NumControverse as label, Titre as Text\n",
        "FROM `poc-bigdata.FluxWeb_Prediction.Controverses_prd`\n",
        "UNION ALL \n",
        "SELECT DISTINCT NumControverse as label, Commentaire as Text\n",
        "FROM `poc-bigdata.FluxWeb_Prediction.Controverses_prd`\n",
        "WHERE Commentaire is not null and length(Commentaire)>5\n",
        "UNION ALL \n",
        "SELECT DISTINCT NumControverse as label, Informations as Text\n",
        "FROM `poc-bigdata.FluxWeb_Prediction.Controverses_prd`\n",
        "WHERE Informations is not null\n",
        "\"\"\"\n",
        "controverses = get_bq_data(q)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqtPfh_EpdkM"
      },
      "source": [
        "#controverses['Text_clean']=controverses.Text.apply(text_preprossing)\n",
        "controverses = controverses[['label','Text']]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "MSdOrlfrz95b",
        "outputId": "d6df17b0-a410-47a4-9f9f-39c501957dd4"
      },
      "source": [
        "print(len(controverses))\n",
        "print(len(controverses.label.unique()))\n",
        "controverses.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3751\n",
            "1408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>389</td>\n",
              "      <td>Le directeur du chantier de Flamanville pour E...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1423</td>\n",
              "      <td>Après les deux premières plaintes déposées à L...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1695</td>\n",
              "      <td>Le fisc français réclamerait 1,6 Md? d'arriéré...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3561</td>\n",
              "      <td>Un ancien directeur des comptes nationaux de P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2912</td>\n",
              "      <td>Suite article du Wall Street Journal du 16/05/...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               Text\n",
              "0    389  Le directeur du chantier de Flamanville pour E...\n",
              "1   1423  Après les deux premières plaintes déposées à L...\n",
              "2   1695  Le fisc français réclamerait 1,6 Md? d'arriéré...\n",
              "3   3561  Un ancien directeur des comptes nationaux de P...\n",
              "4   2912  Suite article du Wall Street Journal du 16/05/..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlIlZ8rDqmg5"
      },
      "source": [
        "### Text classification with an RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z92fgxINqp7E"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow as tf\n",
        "tfds.disable_progress_bar()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-s3i87Mqx9W"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYLWAp_aqyX5",
        "outputId": "81d82c71-3cc2-4807-9327-ab098fe55b28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(controverses, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2400 train examples\n",
            "600 validation examples\n",
            "751 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ5e1rs7tvpI",
        "outputId": "9c6f2980-231c-498f-9987-cedcd0e79d12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1934</th>\n",
              "      <td>2445</td>\n",
              "      <td>L'UFC-Que Choisir vient d'annoncer le dépôt de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3372</th>\n",
              "      <td>2843</td>\n",
              "      <td>L'autorité suédoise de surveillance financière...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>2673</td>\n",
              "      <td>Détournements de fonds de plusieurs filiales s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>3250</td>\n",
              "      <td>Un tribunal de la province de Hebei, dans le n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3016</th>\n",
              "      <td>2836</td>\n",
              "      <td>Les enquêteurs ont remis au bureau du procureu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               Text\n",
              "1934   2445  L'UFC-Que Choisir vient d'annoncer le dépôt de...\n",
              "3372   2843  L'autorité suédoise de surveillance financière...\n",
              "166    2673  Détournements de fonds de plusieurs filiales s...\n",
              "85     3250  Un tribunal de la province de Hebei, dans le n...\n",
              "3016   2836  Les enquêteurs ont remis au bureau du procureu..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNSuVrS9rD5L"
      },
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  #dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('label')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dataframe.values, labels.values))\n",
        "  #ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNdFIf0Asv-Z"
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "train_dataset = df_to_dataset(train)\n",
        "test_dataset = df_to_dataset(test)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkpYs1_6s6rO",
        "outputId": "786ce639-126d-49a9-df3e-0cc4e5a0831f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texts:  [[b\"Les Amis de la Terre d\\xc3\\xa9noncent la Soci\\xc3\\xa9t\\xc3\\xa9 G\\xc3\\xa9n\\xc3\\xa9rale pour son soutien aux projets d'exportation de gaz de schiste liqu\\xc3\\xa9fi\\xc3\\xa9 Rio Grande LNG et de double gazoduc Rio Bravo Pipeline au Texas. La banque servirait en effet de conseiller financier \\xc3\\xa0 NextDecade, soci\\xc3\\xa9t\\xc3\\xa9 portant le projet. L'ONG d\\xc3\\xa9nonce ainsi les impacts n\\xc3\\xa9fastes de ces projets sur l'environnement et les communaut\\xc3\\xa9s locales, qui iraient l'encontre des Principes Equateur dont la banque est signataire. Le projet Rio Grande serait notamment localis\\xc3\\xa9 au coeur d'une r\\xc3\\xa9serve naturelle, et les processus de consultation des communaut\\xc3\\xa9s locales seraient d\\xc3\\xa9ficients. Rappelons que les engagements pris par la banque sur la question des \\xc3\\xa9nergies non conventionnelles avaient \\xc3\\xa9t\\xc3\\xa9 jug\\xc3\\xa9s l\\xc3\\xa9gers par l'ONG, en comparaison notamment \\xc3\\xa0 son homologue BNP Paribas.\"]\n",
            " [b'BMW a d\\xc3\\xa9pos\\xc3\\xa9 une plainte en Allemagne contre deux de ses fournisseurs, le fran\\xc3\\xa7ais Valeo et le japonais Denso, qu\\'il accuse de s\\'\\xc3\\xaatre entendus sur les prix de composants de climatisation. Les dommages et int\\xc3\\xa9r\\xc3\\xaats demand\\xc3\\xa9s atteignent les 141 millions d\\'euros. Valeo a indiqu\\xc3\\xa9 regretter que BMW, un \"partenaire de longue date\", ait d\\xc3\\xa9cid\\xc3\\xa9 d\\'engager une action en justice et s\\'est d\\xc3\\xa9clar\\xc3\\xa9 d\\xc3\\xa9termin\\xc3\\xa9 \\xc3\\xa0 monter une d\\xc3\\xa9fense vigoureuse et \\xc3\\xa0 d\\xc3\\xa9montrer que BMW n\\'a subi aucun pr\\xc3\\xa9judice. Rappelons que la Commission europ\\xc3\\xa9enne a inflig\\xc3\\xa9 des amendes \\xc3\\xa0 plusieurs \\xc3\\xa9quipementiers, dont Valeo, pour un montant total de 155 millions d\\'euros en 2017, affirmant qu\\'ils avaient particip\\xc3\\xa9 \\xc3\\xa0 une ou plusieurs des quatre ententes sur les composants de climatisation et de refroidissement des moteurs. Ayant inform\\xc3\\xa9 l\\'UE de l\\'existence de ces cartels, Denso n\\'avait pas \\xc3\\xa9t\\xc3\\xa9 condamn\\xc3\\xa9 \\xc3\\xa0 une amende.']\n",
            " [b\"Plusieurs organisations dont l'organisation syndicale Solidaires Groupe RATP d\\xc3\\xa9noncent la pollution aux particules fines des tunnels du m\\xc3\\xa9tro parisien et des RER, portant ainsi atteinte \\xc3\\xa0 la sant\\xc3\\xa9 des travailleurs mais aussi \\xc3\\xa0 celle des voyageurs. En effet, une \\xc3\\xa9tude r\\xc3\\xa9cemment r\\xc3\\xa9alis\\xc3\\xa9e par RATP aupr\\xc3\\xa8s de ses agents aurait mis en avant des niveaux d'exposition aux particules fines deux \\xc3\\xa0 quatre fois sup\\xc3\\xa9rieurs aux seuils recommand\\xc3\\xa9s par l'OMS (Organisation mondiale de la sant\\xc3\\xa9). Les personnes les plus expos\\xc3\\xa9es seraient les employ\\xc3\\xa9s travaillant dans les souterrains et le personnel de maintenance, principalement sous-trait\\xc3\\xa9. La haute concentration de particules fines proviendrait de la pollution issue du trafic routier, mais aussi des outils utilis\\xc3\\xa9s pour les travaux de maintenance. Une exposition r\\xc3\\xa9p\\xc3\\xa9t\\xc3\\xa9e aux particules fines peut engendrer des maladies respiratoires et cardio-vasculaires.\"]]\n",
            "\n",
            "labels:  [2505 2452 2816]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl5LtldntAaM"
      },
      "source": [
        "VOCAB_SIZE = 5000\n",
        "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0MHeOQgtTmP",
        "outputId": "1efb08c3-fa7a-4612-ca01-6ba9a6e77ac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'de', 'la', 'des', 'et', 'le', 'les', 'à', 'en', 'a',\n",
              "       'pour', 'du', 'dans', 'une', 'par', 'un', 'sur', 'que', 'au'],\n",
              "      dtype='<U20')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkhmd3sMt_bK"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l65zU86nuY5T",
        "outputId": "42197a12-5b10-4164-9b9a-f6b76cf6c227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False, True, True, True, True]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWm0XyYRubLu",
        "outputId": "44c96783-93f4-4d49-9d96-fade25469eda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01629866]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7rXggQGuovP",
        "outputId": "8e750f17-2b09-4940-bdd6-e102ed633a76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01629867]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icwOLeSdvb7p"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5mVSRuevfCT",
        "outputId": "a7686f86-df17-44b2-b9c5-b35f12e2d283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "val_dataset = df_to_dataset(val)\n",
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - ETA: 0s - loss: -626.2266 - accuracy: 0.0000e+00WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 30 batches). You may need to use the repeat() function when building your dataset.\n",
            "75/75 [==============================] - 28s 280ms/step - loss: -626.2266 - accuracy: 0.0000e+00 - val_loss: -3757.6128 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 18s 244ms/step - loss: -22067.1699 - accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 18s 238ms/step - loss: -60109.5117 - accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 18s 240ms/step - loss: -102574.8828 - accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 19s 248ms/step - loss: -142385.4375 - accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 18s 244ms/step - loss: -181086.9844 - accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 18s 242ms/step - loss: -219893.6250 - accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 19s 251ms/step - loss: -259510.3438 - accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 18s 238ms/step - loss: -300333.0938 - accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 18s 242ms/step - loss: -342610.6875 - accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-a4q5GSvslP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GCIUWL1p56R"
      },
      "source": [
        "### SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6AlgtWqsD7q"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download fr_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBzAA6MZ3Y9I"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp-fCZ9YrIgf"
      },
      "source": [
        "def text_preprossing(text):\n",
        "    tokens = nlp(text)\n",
        "    clean = []\n",
        "    for token in tokens:\n",
        "        if not any([token.is_digit, token.is_punct, token.is_currency,\n",
        "               token.like_url, token.like_num, token.like_email,\n",
        "               token.is_stop]):\n",
        "            clean += [token.lemma_]\n",
        "    #return tf.strings.join(clean, separator=' ')\n",
        "    return ' '.join(clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2CymuS5CteF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "65464648-1aed-4cbd-9fa7-3323a61ae02a"
      },
      "source": [
        "data = pd.DataFrame()\n",
        "data['label'] = controverses.label\n",
        "data['Text'] = controverses.Text.apply(text_preprossing)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2866</td>\n",
              "      <td>révélation scandale comptable survenir impliqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>389</td>\n",
              "      <td>epr Flamanville malfaçon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2543</td>\n",
              "      <td>condamnation suite déversement rivière</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3506</td>\n",
              "      <td>poursuite Etats-Unis lien émission no niveau c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3016</td>\n",
              "      <td>incendie usine Lubrizol Rouen</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               Text\n",
              "0   2866  révélation scandale comptable survenir impliqu...\n",
              "1    389                           epr Flamanville malfaçon\n",
              "2   2543             condamnation suite déversement rivière\n",
              "3   3506  poursuite Etats-Unis lien émission no niveau c...\n",
              "4   3016                      incendie usine Lubrizol Rouen"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLFEyswQuZm4",
        "outputId": "aff0525f-3ebb-4c7d-ecb1-40c9c9068e0f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(data, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2400 train examples\n",
            "600 validation examples\n",
            "751 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieVNljkp5rGf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "a7afb759-27bd-4457-9b05-d028f42d058e"
      },
      "source": [
        "val.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1212</th>\n",
              "      <td>69</td>\n",
              "      <td>procédure cours cadre fait avérer corruption</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3551</th>\n",
              "      <td>3178</td>\n",
              "      <td>ancien avocat filiale conglomérat Danaher Corp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1040</th>\n",
              "      <td>3370</td>\n",
              "      <td>reprise opération Carnival Etats-Unis</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2858</th>\n",
              "      <td>2604</td>\n",
              "      <td>syndicat approuver octobre accord proposer pdg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>1407</td>\n",
              "      <td>rapport critique institution financier finance...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      label                                               Text\n",
              "1212     69       procédure cours cadre fait avérer corruption\n",
              "3551   3178  ancien avocat filiale conglomérat Danaher Corp...\n",
              "1040   3370              reprise opération Carnival Etats-Unis\n",
              "2858   2604  syndicat approuver octobre accord proposer pdg...\n",
              "69     1407  rapport critique institution financier finance..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiovEVoevuJu"
      },
      "source": [
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  #dataframe = dataframe.copy()\n",
        "  #labels = dataframe.pop('label')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dataframe.Text.values, dataframe.label.values))\n",
        "  #ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Sb6DDzwxBb",
        "outputId": "de70d245-b41a-4b36-8d42-af986f47855a"
      },
      "source": [
        "batch_size = 32 # A small batch sized is used for demonstration purposes\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
        "train_ds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bThDbEb-6rRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a877d3e5-ba9c-4d0f-a535-5b0f27f10a5b"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "  for i in range(5):\n",
        "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3319 b'enqu\\xc3\\xaate conduire ICIJ m\\xc3\\xa9dia montre grand banque poreux blanchiment argent base suspiciou activity report SAR transmettre banque autorit\\xc3\\xa9 lutte antiblanchiment am\\xc3\\xa9ricain fincen rapport ultraconfidentiels repr\\xc3\\xa9senter total mds$ transaction suspect r\\xc3\\xa9aliser montrer banque circuler passivement travers compte bancaire personne soci\\xc3\\xa9t\\xc3\\xa9 identifier argent susceptible relever blanchiment aucun banque solliciter souhaiter r\\xc3\\xa9agir fincen proposer septembre vaste r\\xc3\\xa9forme cens\\xc3\\xa9 am\\xc3\\xa9liorer efficacit\\xc3\\xa9 lutte anti-blanchiment renforcer obligation d\\xc3\\xa9claration banque   '\n",
            "3460 b'critique politique \\xc3\\xa9conomique chinois fondateur cons\\xc3\\xa9quence'\n",
            "1330 b'ong n\\xc3\\xa9erlandais milieudefensie enqu\\xc3\\xaater fournisseur huile palme Neste neste acheter mt distillat acide gras palme pfad d\\xc3\\xa9chet production huile palme principalement malaisie indon\\xc3\\xa9sie ong accuse Neste destruction for\\xc3\\xaat tropical achat cours dernier mois fournisseur impliquer incendie feu observer conflit foncier pot-de-vin violation droit travail exemple First Resources Ltd accuser d\\xc3\\xa9fricher hectare for\\xc3\\xaat tropical feu conflit foncier communaut\\xc3\\xa9 local   Golden Agri Resources incendie \\xc3\\xaatre d\\xc3\\xa9nombrer filiale Golden Veroleum Liberia utiliser force arm\\xc3\\xa9e extorquer terrain aupr\\xc3\\xa8s population local plaindre'\n",
            "2485 b'Soci\\xc3\\xa9t\\xc3\\xa9 energie eau Gabon seeg filiale groupe Veolia d\\xc3\\xa9poser mars demande conciliation international demande suite r\\xc3\\xa9quisition actif personnel r\\xc3\\xa9siliation brutal concession service public production transport distribution eau potable \\xc3\\xa9nergie \\xc3\\xa9lectrique r\\xc3\\xa9publique Gabon f\\xc3\\xa9vrier gouvernement invoquer d\\xc3\\xa9gradation qualit\\xc3\\xa9 service rendre usager effort financier consentir etat non suivre effet escompter plainte r\\xc3\\xa9current utilisateur seeg estime r\\xc3\\xa9siliation r\\xc3\\xa9quisition ill\\xc3\\xa9gal causer grave pr\\xc3\\xa9judice'\n",
            "2772 b'malaisie annoncer vendredi Goldman Sachs accepter accord mds$ Mds clore scandale lier fonds souverain 1MDB banque affaire am\\xc3\\xa9ricain verser malaisie mds$ garantir md$ actif lier obligation fonds 1MDB pr\\xc3\\xa9ciser minist\\xc3\\xa8re malaisien finance communiqu\\xc3\\xa9 parquet malaisien engager d\\xc3\\xa9cembre poursuite judiciaire contre unit\\xc3\\xa9 Goldman Sachs accusation tromperie encontre investisseur \\xc3\\xa9mission obligataire organiser banque profit fonds 1mdb total mds$.'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvf5tfl262Fl"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYhU3yhg7ysF"
      },
      "source": [
        "-------  \n",
        "### Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yf38V0Spgg3"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqSEtyDksvBK"
      },
      "source": [
        "vocab_size = 10000\n",
        "sequence_length = 600\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    #standardize= text_preprossing,\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='tf-idf',\n",
        "    #output_sequence_length=sequence_length\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGjzxZGsBxBM"
      },
      "source": [
        "text_list = train.Text.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0k6GOBN50x8"
      },
      "source": [
        "vectorize_layer.adapt(text_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i4oz_7nInN2"
      },
      "source": [
        "vectorize_layer.get_vocabulary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoTERsbu3sRX"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBFZEV8YHyAA"
      },
      "source": [
        "\n",
        "embedding_dim=1000\n",
        "\n",
        "model = Sequential([\n",
        "  vectorize_layer,\n",
        "  Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
        "  GlobalAveragePooling1D(),\n",
        "  Dense(16, activation='relu'),\n",
        "  Dense(1)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUf31CwRH7Ng"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaKzMiSM6k8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b87fa6-c7b0-4790-fda5-e89da133fc6e"
      },
      "source": [
        "model.fit(train_ds, \n",
        "          epochs=40, \n",
        "          validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "75/75 [==============================] - 3s 26ms/step - loss: -779.2183 - accuracy: 0.0000e+00 - val_loss: -1786.3885 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -4096.7046 - accuracy: 0.0000e+00 - val_loss: -7218.8159 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -13170.0039 - accuracy: 0.0000e+00 - val_loss: -20210.8047 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -31861.1738 - accuracy: 0.0000e+00 - val_loss: -44427.7812 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -63834.3125 - accuracy: 0.0000e+00 - val_loss: -83342.9688 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -112532.9219 - accuracy: 0.0000e+00 - val_loss: -140196.9688 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -181148.9375 - accuracy: 0.0000e+00 - val_loss: -217976.3750 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/10\n",
            "75/75 [==============================] - 2s 24ms/step - loss: -272621.8750 - accuracy: 0.0000e+00 - val_loss: -319431.0625 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -389657.6562 - accuracy: 0.0000e+00 - val_loss: -447080.0625 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/10\n",
            "75/75 [==============================] - 2s 23ms/step - loss: -534746.1875 - accuracy: 0.0000e+00 - val_loss: -603253.3125 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5a0eef3910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeSaSkfTILKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3045297b-fa41-4e8a-a147-5e98e1bf3bae"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_vectorization_5 (TextVe (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 10000, 16)         160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,289\n",
            "Trainable params: 160,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u88jdipb6nRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d164790-80f1-4306-a038-bdf21a046521"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24/24 [==============================] - 0s 9ms/step - loss: -602432.0625 - accuracy: 0.0000e+00\n",
            "Accuracy 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eURBDR2KKORo"
      },
      "source": [
        "#### tfidf model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRq0QLFCLAWo"
      },
      "source": [
        "def text_preprossing2(text):\n",
        "    tokens = nlp(text)\n",
        "    clean = []\n",
        "    for token in tokens:\n",
        "        if not any([token.is_digit, token.is_punct, token.is_currency,\n",
        "               token.like_url, token.like_num, token.like_email,\n",
        "               token.is_stop]):\n",
        "            clean += [token.lemma_]\n",
        "    return tf.strings.join(clean, separator=' ')\n",
        "    #return ' '.join(clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nyk7GW7K_uO"
      },
      "source": [
        "vectorize_layer2 = TextVectorization(\n",
        "    #standardize= text_preprossing,\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='tf-idf',\n",
        "    #output_sequence_length=sequence_length\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6SLYUjLJGN"
      },
      "source": [
        "vectorize_layer2.adapt(data.Text.values.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSPsmJTnKNig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b1030b-a8d2-4d05-e17d-7cd6dc631a4f"
      },
      "source": [
        "tfidf = tf.keras.models.Sequential()\n",
        "tfidf.add(tf.keras.Input(shape=(1,),dtype=tf.string))\n",
        "tfidf.add(vectorize_layer2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iECWOpVNMzh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 942
        },
        "outputId": "13c99f4b-c17a-437e-d9ea-fb497efc14d5"
      },
      "source": [
        "tfidf.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tfidf.fit(train.Text.values.tolist(), \n",
        "          epochs=40, \n",
        "          validation_data=val.Text.values.tolist())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-11702403d63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m tfidf.fit(train.Text.values.tolist(), \n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           validation_data=val.Text.values.tolist())\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:838 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:800 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:439 update_state\n        self.build(y_pred, y_true)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:362 build\n        self._metrics, y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1378 map_structure_up_to\n        **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1473 map_structure_with_tuple_paths_up_to\n        func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1473 <listcomp>\n        func(*args, **kwargs) for args in zip(flat_path_gen, *flat_value_gen)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py:1376 <lambda>\n        lambda _, *values: func(*values),  # Discards the path arg.\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:485 _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:485 <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:506 _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    AttributeError: 'NoneType' object has no attribute 'shape'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Wm1OCmL0JU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac74213-8bd2-44fd-e2c4-bdc9871b6ce6"
      },
      "source": [
        "print(test.Text.values[0])\n",
        "tfidf.predict([test.Text.values[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "US Equal Employment Opportunity commission EEOC intenter action justice tribunal fédéral NY contre Walmart enfreindre loi fédéral laisser employé masculin harceler sexuellement collègue travail prendre mesure cesse lieu mettre fin harcèlement Walmart demander employé harceler défendre contraindre salarié démissionner agence fédéral affirme walmart recevoir plainte sujet comportement harcelant homme partir entreprise prendre mesure efficace faire cesser harcèlement eeoc réclame arriéré salaire dommage intérêt compensatoire dommage intérêt punitif employé concerner mesure redressement prévenir potentiel cas harcèlement sexuel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.586464, 0.      , 0.      , ..., 0.      , 0.      , 0.      ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUh5-B936p9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c525bf-1800-402c-ac0a-4a93dfbb7efd"
      },
      "source": [
        "tfidf.save('tf_idf_model')\n",
        "#reloaded_model = tf.keras.models.load_model('my_pet_classifier')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: tf_idf_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hCijeiB6zP7"
      },
      "source": [
        "news =''\n",
        "\n",
        "input_dict = {'Text': tf.convert_to_tensor([news])}\n",
        "predictions = reloaded_model.predict(input_dict)\n",
        "prob = tf.nn.sigmoid(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This particular pet had a %.1f percent probability \"\n",
        "    \"of getting adopted.\" % (100 * prob)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}